{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Reinforcement Learning with Open AI Gym(keras).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aayushrai/Reinforcement_learning/blob/master/Deep_Reinforcement_Learning_with_Open_AI_Gym(keras).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Tt1jZroYKEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports\n",
        "from collections import deque\n",
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import keras\n",
        "import cv2\n",
        "from keras.models import load_model, Sequential\n",
        "from keras.layers.convolutional import Convolution2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers.core import Activation, Dropout, Flatten, Dense\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython import display"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahdovNlYZPqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# memory for store experience\n",
        "class ReplayBuffer:\n",
        "    \"\"\"Constructs a buffer object that stores the past moves\n",
        "    and samples a set of subsamples\"\"\"\n",
        "\n",
        "    def __init__(self, buffer_size):\n",
        "        self.buffer_size = buffer_size\n",
        "        self.count = 0\n",
        "        self.buffer = deque()\n",
        "\n",
        "    def add(self, s, a, r, d, s2):\n",
        "        \"\"\"Add an experience to the buffer\"\"\"\n",
        "        # S represents current state, a is action,\n",
        "        # r is reward, d is whether it is the end, \n",
        "        # and s2 is next state\n",
        "        experience = (s, a, r, d, s2)\n",
        "        if self.count < self.buffer_size:\n",
        "            self.buffer.append(experience)\n",
        "            self.count += 1\n",
        "        else:\n",
        "            self.buffer.popleft()\n",
        "            self.buffer.append(experience)\n",
        "\n",
        "    def size(self):\n",
        "        return self.count\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Samples a total of elements equal to batch_size from buffer\n",
        "        if buffer contains enough elements. Otherwise return all elements\"\"\"\n",
        "\n",
        "        batch = []\n",
        "\n",
        "        if self.count < batch_size:\n",
        "            batch = random.sample(self.buffer, self.count)\n",
        "        else:\n",
        "            batch = random.sample(self.buffer, batch_size)\n",
        "\n",
        "        # Maps each experience in batch in batches of states, actions, rewards\n",
        "        # and new states\n",
        "        s_batch, a_batch, r_batch, d_batch, s2_batch = list(map(np.array, list(zip(*batch))))\n",
        "\n",
        "        return s_batch, a_batch, r_batch, d_batch, s2_batch\n",
        "\n",
        "    def clear(self):\n",
        "        self.buffer.clear()\n",
        "        self.count = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FyZKgO_b_zZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DECAY_RATE = 0.99\n",
        "BUFFER_SIZE = 40000\n",
        "MINIBATCH_SIZE = 64\n",
        "TOT_FRAME = 3000000\n",
        "EPSILON_DECAY = 1000000\n",
        "MIN_OBSERVATION = 5000\n",
        "FINAL_EPSILON = 0.05\n",
        "INITIAL_EPSILON = 0.1\n",
        "NUM_ACTIONS = 6\n",
        "TAU = 0.01\n",
        "# Number of frames to throw into network\n",
        "NUM_FRAMES = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVpI7TWGc_3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DeepQ(object):\n",
        "    \"\"\"Constructs the desired deep q learning network\"\"\"\n",
        "    def __init__(self):\n",
        "        self.construct_q_network()\n",
        "\n",
        "    def construct_q_network(self):\n",
        "        # Uses the network architecture found in DeepMind paper\n",
        "        self.model = Sequential()\n",
        "        self.model.add(Convolution2D(32, 8, 8, subsample=(4, 4), input_shape=(84, 84, NUM_FRAMES)))\n",
        "        self.model.add(Activation('relu'))\n",
        "        self.model.add(Convolution2D(64, 4, 4, subsample=(2, 2)))\n",
        "        self.model.add(Activation('relu'))\n",
        "        self.model.add(Convolution2D(64, 3, 3))\n",
        "        self.model.add(Activation('relu'))\n",
        "        self.model.add(Flatten())\n",
        "        self.model.add(Dense(512))\n",
        "        self.model.add(Activation('relu'))\n",
        "        self.model.add(Dense(NUM_ACTIONS))\n",
        "        self.model.compile(loss='mse', optimizer=Adam(lr=0.00001))\n",
        "\n",
        "        # Creates a target network as described in DeepMind paper\n",
        "        self.target_model = Sequential()\n",
        "        self.target_model.add(Convolution2D(32, 8, 8, subsample=(4, 4), input_shape=(84, 84, NUM_FRAMES)))\n",
        "        self.target_model.add(Activation('relu'))\n",
        "        self.target_model.add(Convolution2D(64, 4, 4, subsample=(2, 2)))\n",
        "        self.target_model.add(Activation('relu'))\n",
        "        self.target_model.add(Convolution2D(64, 3, 3))\n",
        "        self.target_model.add(Activation('relu'))\n",
        "        self.target_model.add(Flatten())\n",
        "        self.target_model.add(Dense(512))\n",
        "        self.model.add(Activation('relu'))\n",
        "        self.target_model.add(Dense(NUM_ACTIONS))\n",
        "        self.target_model.compile(loss='mse', optimizer=Adam(lr=0.00001))\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "    def save_network(self, path):\n",
        "        # Saves model at specified path as h5 file\n",
        "        self.model.save(path)\n",
        "        print(\"Successfully saved network.\")\n",
        "\n",
        "    def load_network(self, path):\n",
        "        self.model = load_model(path)\n",
        "        print(\"Succesfully loaded network.\")\n",
        "\n",
        "    def target_train(self):\n",
        "        model_weights = self.model.get_weights()\n",
        "        target_model_weights = self.target_model.get_weights()\n",
        "        for i in range(len(model_weights)):\n",
        "            target_model_weights[i] = TAU * model_weights[i] + (1 - TAU) * target_model_weights[i]\n",
        "        self.target_model.set_weights(target_model_weights)\n",
        "\n",
        "    def predict_movement(self, data, epsilon):\n",
        "        \"\"\"Predict movement of game controler where is epsilon\n",
        "        probability randomly move.\"\"\"\n",
        "        q_actions = self.model.predict(data.reshape(1, 84, 84, NUM_FRAMES), batch_size = 1)\n",
        "        opt_policy = np.argmax(q_actions)\n",
        "        rand_val = np.random.random()\n",
        "        if rand_val < epsilon:\n",
        "            opt_policy = np.random.randint(0, NUM_ACTIONS)\n",
        "        return opt_policy, q_actions[0, opt_policy]\n",
        "    \n",
        "    def predict_movement(self, data, epsilon):\n",
        "        \"\"\"Predict movement of game controler where is epsilon\n",
        "        probability randomly move.\"\"\"\n",
        "        q_actions = self.model.predict(data.reshape(1, 84, 84, NUM_FRAMES), batch_size = 1)\n",
        "        opt_policy = np.argmax(q_actions)\n",
        "        rand_val = np.random.random()\n",
        "        if rand_val < epsilon:\n",
        "            opt_policy = np.random.randint(0, NUM_ACTIONS)\n",
        "        return opt_policy, q_actions[0, opt_policy]\n",
        "\n",
        "    def train(self, s_batch, a_batch, r_batch, d_batch, s2_batch, observation_num):\n",
        "        \"\"\"Trains network to fit given parameters\"\"\"\n",
        "        batch_size = s_batch.shape[0]\n",
        "        targets = np.zeros((batch_size, NUM_ACTIONS))\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            targets[i] = self.model.predict(s_batch[i].reshape(1, 84, 84, NUM_FRAMES), batch_size = 1)\n",
        "            fut_action = self.target_model.predict(s2_batch[i].reshape(1, 84, 84, NUM_FRAMES), batch_size = 1)\n",
        "            targets[i, a_batch[i]] = r_batch[i]\n",
        "            if d_batch[i] == False:\n",
        "                targets[i, a_batch[i]] += DECAY_RATE * np.max(fut_action)\n",
        "\n",
        "        loss = self.model.train_on_batch(s_batch, targets)\n",
        "\n",
        "        # Print the loss every 10 iterations.\n",
        "        if observation_num % 10 == 0:\n",
        "            print(\"We had a loss equal to \", loss)\n",
        "\n",
        "    def save_network(self, path):\n",
        "        # Saves model at specified path as h5 file\n",
        "        self.model.save(path)\n",
        "        print(\"Successfully saved network.\")\n",
        "\n",
        "    def load_network(self, path):\n",
        "        self.model = load_model(path)\n",
        "        print(\"Succesfully loaded network.\")\n",
        "\n",
        "    def target_train(self):\n",
        "        model_weights = self.model.get_weights()\n",
        "        target_model_weights = self.target_model.get_weights()\n",
        "        for i in range(len(model_weights)):\n",
        "            target_model_weights[i] = TAU * model_weights[i] + (1 - TAU) * target_model_weights[i]\n",
        "        self.target_model.set_weights(target_model_weights)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvN8S0MCdAZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SpaceInvader(object):\n",
        "\n",
        "    def __init__(self, mode):\n",
        "        self.env = gym.make('SpaceInvaders-v0')\n",
        "        self.env.reset()\n",
        "        self.replay_buffer = ReplayBuffer(BUFFER_SIZE)\n",
        "\n",
        "        # Construct appropriate network based on flags\n",
        "        if mode == \"DDQN\":\n",
        "            self.deep_q = DeepQ()\n",
        "        elif mode == \"DQN\":\n",
        "            self.deep_q = DuelQ()\n",
        "\n",
        "        # A buffer that keeps the last 3 images\n",
        "        self.process_buffer = []\n",
        "        # Initialize buffer with the first frame\n",
        "        s1, r1, _, _ = self.env.step(0)\n",
        "        s2, r2, _, _ = self.env.step(0)\n",
        "        s3, r3, _, _ = self.env.step(0)\n",
        "        self.process_buffer = [s1, s2, s3]\n",
        "\n",
        "    def load_network(self, path):\n",
        "        self.deep_q.load_network(path)\n",
        "\n",
        "    def convert_process_buffer(self):\n",
        "        \"\"\"Converts the list of NUM_FRAMES images in the process buffer\n",
        "        into one training sample\"\"\"\n",
        "        black_buffer = [cv2.resize(cv2.cvtColor(x, cv2.COLOR_RGB2GRAY), (84, 90)) for x in self.process_buffer]\n",
        "        black_buffer = [x[1:85, :, np.newaxis] for x in black_buffer]\n",
        "        return np.concatenate(black_buffer, axis=2)\n",
        "\n",
        "    def train(self, num_frames):\n",
        "        observation_num = 0\n",
        "        curr_state = self.convert_process_buffer()\n",
        "        epsilon = INITIAL_EPSILON\n",
        "        alive_frame = 0\n",
        "        total_reward = 0\n",
        "\n",
        "        while observation_num < num_frames:\n",
        "            if observation_num % 1000 == 999:\n",
        "                print((\"Executing loop %d\" %observation_num))\n",
        "\n",
        "            # Slowly decay the learning rate\n",
        "            if epsilon > FINAL_EPSILON:\n",
        "                epsilon -= (INITIAL_EPSILON-FINAL_EPSILON)/EPSILON_DECAY\n",
        "\n",
        "            initial_state = self.convert_process_buffer()\n",
        "            self.process_buffer = []\n",
        "\n",
        "            predict_movement, predict_q_value = self.deep_q.predict_movement(curr_state, epsilon)\n",
        "\n",
        "            reward, done = 0, False\n",
        "            for i in range(NUM_FRAMES):\n",
        "                temp_observation, temp_reward, temp_done, _ = self.env.step(predict_movement)\n",
        "                reward += temp_reward\n",
        "                self.process_buffer.append(temp_observation)\n",
        "                done = done | temp_done\n",
        "\n",
        "            if observation_num % 10 == 0:\n",
        "                print(\"We predicted a q value of \", predict_q_value)\n",
        "\n",
        "            if done:\n",
        "                print(\"Lived with maximum time \", alive_frame)\n",
        "                print(\"Earned a total of reward equal to \", total_reward)\n",
        "                self.env.reset()\n",
        "                alive_frame = 0\n",
        "                total_reward = 0\n",
        "\n",
        "            new_state = self.convert_process_buffer()\n",
        "            self.replay_buffer.add(initial_state, predict_movement, reward, done, new_state)\n",
        "            total_reward += reward\n",
        "\n",
        "            if self.replay_buffer.size() > MIN_OBSERVATION:\n",
        "                s_batch, a_batch, r_batch, d_batch, s2_batch = self.replay_buffer.sample(MINIBATCH_SIZE)\n",
        "                self.deep_q.train(s_batch, a_batch, r_batch, d_batch, s2_batch, observation_num)\n",
        "                self.deep_q.target_train()\n",
        "\n",
        "            # Save the network every 100000 iterations\n",
        "            if observation_num % 10000 == 9999:\n",
        "                print(\"Saving Network\")\n",
        "                self.deep_q.save_network(\"saved.h5\")\n",
        "\n",
        "            alive_frame += 1\n",
        "            observation_num += 1\n",
        "\n",
        "    def simulate(self, path = \"\", save = False):\n",
        "        \"\"\"Simulates game\"\"\"\n",
        "        done = False\n",
        "        tot_award = 0\n",
        "        if save:\n",
        "            self.env.monitor.start(path, force=True)\n",
        "        self.env.reset()\n",
        "        self.env.render()\n",
        "        while not done:\n",
        "            state = self.convert_process_buffer()\n",
        "            predict_movement = self.deep_q.predict_movement(state, 0)[0]\n",
        "            self.env.render()\n",
        "            observation, reward, done, _ = self.env.step(predict_movement)\n",
        "            tot_award += reward\n",
        "            self.process_buffer.append(observation)\n",
        "            self.process_buffer = self.process_buffer[1:]\n",
        "        if save:\n",
        "            self.env.monitor.close()\n",
        "    def simulate2(self, path = \"\", save = False):\n",
        "        \"\"\"Simulates game\"\"\"\n",
        "        done = False\n",
        "        tot_award = 0\n",
        "        if save:\n",
        "          self.env.monitor.start(path, force=True)\n",
        "        self.env.reset()\n",
        "        self.show_state(self.env)\n",
        "        while not done:\n",
        "            state = self.convert_process_buffer()\n",
        "            predict_movement = self.deep_q.predict_movement(state, 0)[0]\n",
        "            self.show_state(self.env)\n",
        "            observation, reward, done, _ = self.env.step(predict_movement)\n",
        "            tot_award += reward\n",
        "            self.process_buffer.append(observation)\n",
        "            self.process_buffer = self.process_buffer[1:]\n",
        "        if save:\n",
        "            self.env.monitor.close()\n",
        "\n",
        "    def show_state(self,env, step=0, info=\"\"):\n",
        "        plt.figure(3)\n",
        "        plt.clf()\n",
        "        plt.imshow(env.render(mode='rgb_array'))\n",
        "        plt.axis('off')\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(plt.gcf())\n",
        "\n",
        "    def calculate_mean(self, num_samples = 100):\n",
        "        reward_list = []\n",
        "        print(\"Printing scores of each trial\")\n",
        "        for i in range(num_samples):\n",
        "            done = False\n",
        "            tot_award = 0\n",
        "            self.env.reset()\n",
        "            while not done:\n",
        "                state = self.convert_process_buffer()\n",
        "                predict_movement = self.deep_q.predict_movement(state, 0.0)[0]\n",
        "                observation, reward, done, _ = self.env.step(predict_movement)\n",
        "                tot_award += reward\n",
        "                self.process_buffer.append(observation)\n",
        "                self.process_buffer = self.process_buffer[1:]\n",
        "            print(tot_award)\n",
        "            reward_list.append(tot_award)\n",
        "        return np.mean(reward_list), np.std(reward_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJb9SD5pmyG-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "outputId": "a8fe1e4d-f1ff-4edd-8274-55baaa32fc57"
      },
      "source": [
        "game_instance = SpaceInvader(\"DDQN\")"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), input_shape=(84, 84, 3..., strides=(4, 4))`\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (4, 4), strides=(2, 2))`\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), input_shape=(84, 84, 3..., strides=(4, 4))`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (4, 4), strides=(2, 2))`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdNMWtybnLhk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "93f22cb8-50c2-4747-9e88-fdddcd082c0e"
      },
      "source": [
        " NUM_FRAME = 100\n",
        " game_instance.train(NUM_FRAME)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We predicted a q value of  7.4775476\n",
            "We predicted a q value of  7.4775476\n",
            "We predicted a q value of  7.4775476\n",
            "We predicted a q value of  7.4775476\n",
            "We predicted a q value of  7.4775476\n",
            "We predicted a q value of  7.4775476\n",
            "We predicted a q value of  7.4775476\n",
            "We predicted a q value of  7.4775476\n",
            "We predicted a q value of  7.4775476\n",
            "We predicted a q value of  7.4775476\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8VBIahKLRsF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "2ebb64c8-f7a4-472a-8c8d-56eb78e09c0b"
      },
      "source": [
        "game_instance.load_network(\"/content/saved.h5\")"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py:1103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(name=\"convolution2d_1\", activity_regularizer=None, trainable=True, input_dtype=\"float32\", batch_input_shape=[None, 84,..., activation=\"linear\", kernel_size=(8, 8), filters=32, strides=[4, 4], padding=\"valid\", data_format=\"channels_last\", kernel_initializer=\"glorot_uniform\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
            "  return cls(**config)\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py:1103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(name=\"convolution2d_2\", activity_regularizer=None, trainable=True, activation=\"linear\", kernel_size=(4, 4), filters=64, strides=[2, 2], padding=\"valid\", data_format=\"channels_last\", kernel_initializer=\"glorot_uniform\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
            "  return cls(**config)\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py:1103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(name=\"convolution2d_3\", activity_regularizer=None, trainable=True, activation=\"linear\", kernel_size=(3, 3), filters=64, strides=[1, 1], padding=\"valid\", data_format=\"channels_last\", kernel_initializer=\"glorot_uniform\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
            "  return cls(**config)\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py:1103: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(name=\"dense_1\", activity_regularizer=None, trainable=True, input_dim=3136, activation=\"linear\", units=256, kernel_initializer=\"glorot_uniform\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
            "  return cls(**config)\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py:1103: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(name=\"dense_2\", activity_regularizer=None, trainable=True, input_dim=256, activation=\"linear\", units=6, kernel_initializer=\"glorot_uniform\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
            "  return cls(**config)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Succesfully loaded network.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:350: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "  warnings.warn('Error in loading the saved optimizer '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KauAqxeso7vv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "64124415-86be-48d8-853b-b46efcc0ab5f"
      },
      "source": [
        "stat = game_instance.calculate_mean()\n",
        "print(\"Game Statistics\")\n",
        "print(stat)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Printing scores of each trial\n",
            "15.0\n",
            "5.0\n",
            "20.0\n",
            "10.0\n",
            "25.0\n",
            "10.0\n",
            "10.0\n",
            "70.0\n",
            "20.0\n",
            "10.0\n",
            "0.0\n",
            "10.0\n",
            "45.0\n",
            "15.0\n",
            "20.0\n",
            "25.0\n",
            "60.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-8f40eb76a6dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Game Statistics\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-cc501169e5c2>\u001b[0m in \u001b[0;36mcalculate_mean\u001b[0;34m(self, num_samples)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_process_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                 \u001b[0mpredict_movement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeep_q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_movement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m                 \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_movement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mtot_award\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-5350f78d1f74>\u001b[0m in \u001b[0;36mpredict_movement\u001b[0;34m(self, data, epsilon)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \"\"\"Predict movement of game controler where is epsilon\n\u001b[1;32m     63\u001b[0m         probability randomly move.\"\"\"\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mq_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m84\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m84\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_FRAMES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mopt_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mrand_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1399\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1401\u001b[0;31m                                             callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m     def train_on_batch(self, x, y,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2915\u001b[0m                 array_vals.append(\n\u001b[1;32m   2916\u001b[0m                     np.asarray(value,\n\u001b[0;32m-> 2917\u001b[0;31m                                dtype=tf.as_dtype(tensor.dtype).as_numpy_dtype))\n\u001b[0m\u001b[1;32m   2918\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2919\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SV8SX7ypWAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#game_instance.simulate2(path=\"n.mp4\", save=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0r9E1H1hp2Bt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "057b1e57-500c-400e-c04a-79e74ea7fa08"
      },
      "source": [
        "game_instance.simulate2()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAALQAAADnCAYAAAC313xrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlRbJ3wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAALQAAADnCAYAAAC313xrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlRbJ3wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UX6hJpo4qGWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}